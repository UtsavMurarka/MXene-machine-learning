{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MXene_NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWvbybVzhdS0",
        "colab_type": "code",
        "outputId": "d07d730c-82fa-4a32-c276-abbf18e2058d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#data=pd.read_csv(\"C:\\\\Users\\\\Utsav\\\\Desktop\\\\Mxene\\\\ML_exp\", sep=',',header=0)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/UtsavMurarka/MXene-machine-learning/master/anant_data_miner/post_midsem_work/data_physical.csv\"\n",
        "\n",
        "data=pd.read_csv(url, sep=',',header=0)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>bandgap</th>\n",
              "      <th>nn1</th>\n",
              "      <th>nn2</th>\n",
              "      <th>nn3</th>\n",
              "      <th>nn4</th>\n",
              "      <th>nn5</th>\n",
              "      <th>np1</th>\n",
              "      <th>np2</th>\n",
              "      <th>np3</th>\n",
              "      <th>np4</th>\n",
              "      <th>np5</th>\n",
              "      <th>...</th>\n",
              "      <th>en2</th>\n",
              "      <th>en3</th>\n",
              "      <th>en4</th>\n",
              "      <th>en5</th>\n",
              "      <th>ie1</th>\n",
              "      <th>ie2</th>\n",
              "      <th>ie3</th>\n",
              "      <th>ie4</th>\n",
              "      <th>ie5</th>\n",
              "      <th>d1</th>\n",
              "      <th>d2</th>\n",
              "      <th>d3</th>\n",
              "      <th>d4</th>\n",
              "      <th>d5</th>\n",
              "      <th>mp1</th>\n",
              "      <th>mp2</th>\n",
              "      <th>mp3</th>\n",
              "      <th>mp4</th>\n",
              "      <th>mp5</th>\n",
              "      <th>bp1</th>\n",
              "      <th>bp2</th>\n",
              "      <th>bp3</th>\n",
              "      <th>bp4</th>\n",
              "      <th>bp5</th>\n",
              "      <th>numiso1</th>\n",
              "      <th>numiso2</th>\n",
              "      <th>numiso3</th>\n",
              "      <th>numiso4</th>\n",
              "      <th>numiso5</th>\n",
              "      <th>shell1</th>\n",
              "      <th>shell2</th>\n",
              "      <th>shell3</th>\n",
              "      <th>shell4</th>\n",
              "      <th>shell5</th>\n",
              "      <th>spheat1</th>\n",
              "      <th>spheat2</th>\n",
              "      <th>spheat3</th>\n",
              "      <th>spheat4</th>\n",
              "      <th>spheat5</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e-16</td>\n",
              "      <td>32.8</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.328</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.55</td>\n",
              "      <td>2.96</td>\n",
              "      <td>2.96</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>11.2603</td>\n",
              "      <td>11.8138</td>\n",
              "      <td>11.8138</td>\n",
              "      <td>7.15</td>\n",
              "      <td>7.15</td>\n",
              "      <td>2.27</td>\n",
              "      <td>3.12000</td>\n",
              "      <td>3.12000</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>3948.15</td>\n",
              "      <td>266.05</td>\n",
              "      <td>266.050</td>\n",
              "      <td>2944</td>\n",
              "      <td>2944</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>332.00</td>\n",
              "      <td>332.00</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>2.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-2.000000e-16</td>\n",
              "      <td>35.6</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.244</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.55</td>\n",
              "      <td>2.96</td>\n",
              "      <td>2.20</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>11.2603</td>\n",
              "      <td>11.8138</td>\n",
              "      <td>13.5984</td>\n",
              "      <td>7.15</td>\n",
              "      <td>7.15</td>\n",
              "      <td>2.27</td>\n",
              "      <td>3.12000</td>\n",
              "      <td>0.00009</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>3948.15</td>\n",
              "      <td>266.05</td>\n",
              "      <td>14.175</td>\n",
              "      <td>2944</td>\n",
              "      <td>2944</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>332.00</td>\n",
              "      <td>20.28</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.474</td>\n",
              "      <td>14.304</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>2.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000e-16</td>\n",
              "      <td>-2.000000e-16</td>\n",
              "      <td>33.1</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.330</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>45</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.16</td>\n",
              "      <td>2.96</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>11.2603</td>\n",
              "      <td>12.9676</td>\n",
              "      <td>11.8138</td>\n",
              "      <td>7.15</td>\n",
              "      <td>7.15</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.00321</td>\n",
              "      <td>3.12000</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>3948.15</td>\n",
              "      <td>172.31</td>\n",
              "      <td>266.050</td>\n",
              "      <td>2944</td>\n",
              "      <td>2944</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>239.11</td>\n",
              "      <td>332.00</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-1.000000e-16</td>\n",
              "      <td>34.6</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.328</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.16</td>\n",
              "      <td>3.16</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>11.2603</td>\n",
              "      <td>12.9676</td>\n",
              "      <td>12.9676</td>\n",
              "      <td>7.15</td>\n",
              "      <td>7.15</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.00321</td>\n",
              "      <td>0.00321</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>3948.15</td>\n",
              "      <td>172.31</td>\n",
              "      <td>172.310</td>\n",
              "      <td>2944</td>\n",
              "      <td>2944</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>239.11</td>\n",
              "      <td>239.11</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000e-16</td>\n",
              "      <td>36.4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.369</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.245</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.16</td>\n",
              "      <td>2.20</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>6.7665</td>\n",
              "      <td>11.2603</td>\n",
              "      <td>12.9676</td>\n",
              "      <td>13.5984</td>\n",
              "      <td>7.15</td>\n",
              "      <td>7.15</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.00321</td>\n",
              "      <td>0.00009</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>2130.15</td>\n",
              "      <td>3948.15</td>\n",
              "      <td>172.31</td>\n",
              "      <td>14.175</td>\n",
              "      <td>2944</td>\n",
              "      <td>2944</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>239.11</td>\n",
              "      <td>20.28</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.479</td>\n",
              "      <td>14.304</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     f1   f2   f3    f4    f5  ...  spheat2  spheat3  spheat4  spheat5  label\n",
              "0  3.26  0.0  0.0 -1.63  2.82  ...    0.449    0.709    0.474    0.474      0\n",
              "1  3.13  0.0  0.0 -1.56  2.71  ...    0.449    0.709    0.474   14.304      0\n",
              "2  3.24  0.0  0.0 -1.62  2.81  ...    0.449    0.709    0.479    0.474      0\n",
              "3  3.18  0.0  0.0 -1.59  2.75  ...    0.449    0.709    0.479    0.479      0\n",
              "4  3.10  0.0  0.0 -1.55  2.68  ...    0.449    0.709    0.479   14.304      0\n",
              "\n",
              "[5 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvaaizy6hlZW",
        "colab_type": "code",
        "outputId": "4df763d3-87a9-4a95-ccf1-360fe3ae53ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#drop band gap column\n",
        "data = data.drop('bandgap', axis=1)\n",
        "print(np.shape(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3079, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bexoTzRMhnvs",
        "colab_type": "code",
        "outputId": "4f7cc020-8449-455b-b665-91b905656721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "mxene=data.values\n",
        "mxene=np.array(mxene)\n",
        "#split the data\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "train=np.array(train)\n",
        "test=np.array(test)\n",
        "NegativeCount=0\n",
        "PositiveCount=0\n",
        "for i in range(len(train)):\n",
        "    if train[i][99]==0 :\n",
        "        NegativeCount=NegativeCount+1\n",
        "    if train[i][99]==1:\n",
        "        PositiveCount=PositiveCount+1\n",
        "print(NegativeCount)\n",
        "print(PositiveCount)\n",
        "print(PositiveCount+NegativeCount)\n",
        "positives=np.zeros((PositiveCount,100))\n",
        "negatives=np.zeros((NegativeCount,100))\n",
        "    \n",
        "j=0\n",
        "k=0\n",
        "    \n",
        "for i in range(len(train)):\n",
        "\n",
        "    if(train[i,99] == 1):\n",
        "        positives[j,:] = train[i,:]\n",
        "        j=j+1\n",
        "    if(train[i,99] == 0):\n",
        "        negatives[k,:] = train[i,:]\n",
        "        k=k+1\n",
        "\n",
        "print(np.shape(train))\n",
        "print(np.shape(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2152\n",
            "311\n",
            "2463\n",
            "(2463, 100)\n",
            "(616, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS9wydbBhqU0",
        "colab_type": "code",
        "outputId": "34958879-5fec-46aa-ffee-4827a74853cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_train=np.zeros(len(train))\n",
        "y_test=np.zeros(len(test))\n",
        "\n",
        "for i in range(len(test)):\n",
        "    y_test[i]=test[i][99]\n",
        "    test[i][99]=1\n",
        "\n",
        "for i in range(len(train)):\n",
        "    y_train[i]=train[i][99]\n",
        "    train[i][99]=1\n",
        "\n",
        "train = np.delete(train, 99, 1)\n",
        "test = np.delete(test, 99, 1)\n",
        "\n",
        "train = np.delete(train, 98, 1)\n",
        "test = np.delete(test, 98, 1)\n",
        "\n",
        "print(np.shape(train))\n",
        "print(np.shape(test))\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train)\n",
        "test_scaled = scaler.transform(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2463, 98)\n",
            "(616, 98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Shct0Wwhs0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train)\n",
        "test_scaled = scaler.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5cd__8lhwOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(98, input_dim=98, activation='relu'))\n",
        "model.add(Dense(10, activation='sigmoid'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJiTBKkKcjVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebAipyqbdxpb",
        "colab_type": "code",
        "outputId": "b1742c26-defc-4516-eb0f-92540abeb39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "model.fit(train_scaled, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2463/2463 [==============================] - 0s 139us/step - loss: 0.0852 - acc: 0.9728\n",
            "Epoch 2/10\n",
            "2463/2463 [==============================] - 0s 136us/step - loss: 0.0845 - acc: 0.9716\n",
            "Epoch 3/10\n",
            "2463/2463 [==============================] - 0s 136us/step - loss: 0.0845 - acc: 0.9691\n",
            "Epoch 4/10\n",
            "2463/2463 [==============================] - 0s 133us/step - loss: 0.0856 - acc: 0.9691\n",
            "Epoch 5/10\n",
            "2463/2463 [==============================] - 0s 138us/step - loss: 0.0802 - acc: 0.9728\n",
            "Epoch 6/10\n",
            "2463/2463 [==============================] - 0s 144us/step - loss: 0.0798 - acc: 0.9720\n",
            "Epoch 7/10\n",
            "2463/2463 [==============================] - 0s 134us/step - loss: 0.0762 - acc: 0.9756\n",
            "Epoch 8/10\n",
            "2463/2463 [==============================] - 0s 137us/step - loss: 0.0776 - acc: 0.9720\n",
            "Epoch 9/10\n",
            "2463/2463 [==============================] - 0s 141us/step - loss: 0.0748 - acc: 0.9740\n",
            "Epoch 10/10\n",
            "2463/2463 [==============================] - 0s 133us/step - loss: 0.0761 - acc: 0.9732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6eb4cf7f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QU6aveqd8l3",
        "colab_type": "code",
        "outputId": "62508ddc-98e7-4034-9d19-ef73292be028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "_, accuracy = model.evaluate(test_scaled, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "616/616 [==============================] - 0s 35us/step\n",
            "Accuracy: 91.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9ckTzv2eURZ",
        "colab_type": "code",
        "outputId": "e84d6b68-fe8a-41af-9eb8-44dfdbebf9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "_, accuracy = model.evaluate(train_scaled, y_train)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2463/2463 [==============================] - 0s 22us/step\n",
            "Accuracy: 98.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qm-S6nMeZ5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.3, input_shape=(98,)))\n",
        "model.add(Dense(98, input_dim=98, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrHy-Hpiytw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOnN8BDvjJOP",
        "colab_type": "code",
        "outputId": "656fb142-4c58-418c-f525-e5396fba8ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_scaled, y_train, epochs=150, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "2463/2463 [==============================] - 2s 642us/step - loss: 0.3950 - acc: 0.8628\n",
            "Epoch 2/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.3501 - acc: 0.8725\n",
            "Epoch 3/150\n",
            "2463/2463 [==============================] - 0s 170us/step - loss: 0.3353 - acc: 0.8701\n",
            "Epoch 4/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.3163 - acc: 0.8745\n",
            "Epoch 5/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.3196 - acc: 0.8827\n",
            "Epoch 6/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.3238 - acc: 0.8725\n",
            "Epoch 7/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.3076 - acc: 0.8733\n",
            "Epoch 8/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2961 - acc: 0.8749\n",
            "Epoch 9/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2962 - acc: 0.8733\n",
            "Epoch 10/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2984 - acc: 0.8782\n",
            "Epoch 11/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2878 - acc: 0.8810\n",
            "Epoch 12/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2881 - acc: 0.8770\n",
            "Epoch 13/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2833 - acc: 0.8794\n",
            "Epoch 14/150\n",
            "2463/2463 [==============================] - 0s 194us/step - loss: 0.2812 - acc: 0.8782\n",
            "Epoch 15/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2768 - acc: 0.8863\n",
            "Epoch 16/150\n",
            "2463/2463 [==============================] - 0s 187us/step - loss: 0.2833 - acc: 0.8766\n",
            "Epoch 17/150\n",
            "2463/2463 [==============================] - 0s 191us/step - loss: 0.2806 - acc: 0.8859\n",
            "Epoch 18/150\n",
            "2463/2463 [==============================] - 0s 183us/step - loss: 0.2735 - acc: 0.8827\n",
            "Epoch 19/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2747 - acc: 0.8827\n",
            "Epoch 20/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2733 - acc: 0.8810\n",
            "Epoch 21/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2692 - acc: 0.8798\n",
            "Epoch 22/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2608 - acc: 0.8896\n",
            "Epoch 23/150\n",
            "2463/2463 [==============================] - 0s 174us/step - loss: 0.2663 - acc: 0.8908\n",
            "Epoch 24/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2554 - acc: 0.8916\n",
            "Epoch 25/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2592 - acc: 0.8944\n",
            "Epoch 26/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2731 - acc: 0.8867\n",
            "Epoch 27/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2636 - acc: 0.8920\n",
            "Epoch 28/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2562 - acc: 0.8965\n",
            "Epoch 29/150\n",
            "2463/2463 [==============================] - 0s 174us/step - loss: 0.2552 - acc: 0.8993\n",
            "Epoch 30/150\n",
            "2463/2463 [==============================] - 0s 172us/step - loss: 0.2505 - acc: 0.9001\n",
            "Epoch 31/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2408 - acc: 0.8973\n",
            "Epoch 32/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2555 - acc: 0.8863\n",
            "Epoch 33/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2494 - acc: 0.8997\n",
            "Epoch 34/150\n",
            "2463/2463 [==============================] - 0s 182us/step - loss: 0.2589 - acc: 0.8851\n",
            "Epoch 35/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2539 - acc: 0.8912\n",
            "Epoch 36/150\n",
            "2463/2463 [==============================] - 0s 173us/step - loss: 0.2526 - acc: 0.8916\n",
            "Epoch 37/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2514 - acc: 0.8916\n",
            "Epoch 38/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2549 - acc: 0.8924\n",
            "Epoch 39/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2395 - acc: 0.8965\n",
            "Epoch 40/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2444 - acc: 0.8957\n",
            "Epoch 41/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2292 - acc: 0.9022\n",
            "Epoch 42/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2389 - acc: 0.8977\n",
            "Epoch 43/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2422 - acc: 0.8952\n",
            "Epoch 44/150\n",
            "2463/2463 [==============================] - 0s 174us/step - loss: 0.2484 - acc: 0.8961\n",
            "Epoch 45/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2398 - acc: 0.8924\n",
            "Epoch 46/150\n",
            "2463/2463 [==============================] - 0s 192us/step - loss: 0.2360 - acc: 0.8952\n",
            "Epoch 47/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2342 - acc: 0.9009\n",
            "Epoch 48/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2402 - acc: 0.8969\n",
            "Epoch 49/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2335 - acc: 0.8981\n",
            "Epoch 50/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.2389 - acc: 0.8993\n",
            "Epoch 51/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2344 - acc: 0.8981\n",
            "Epoch 52/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2345 - acc: 0.8965\n",
            "Epoch 53/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2398 - acc: 0.8973\n",
            "Epoch 54/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2339 - acc: 0.8961\n",
            "Epoch 55/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2374 - acc: 0.9034\n",
            "Epoch 56/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2337 - acc: 0.9001\n",
            "Epoch 57/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2418 - acc: 0.8948\n",
            "Epoch 58/150\n",
            "2463/2463 [==============================] - 0s 190us/step - loss: 0.2284 - acc: 0.9009\n",
            "Epoch 59/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2234 - acc: 0.9066\n",
            "Epoch 60/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.2301 - acc: 0.9013\n",
            "Epoch 61/150\n",
            "2463/2463 [==============================] - 0s 183us/step - loss: 0.2388 - acc: 0.9013\n",
            "Epoch 62/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2324 - acc: 0.9050\n",
            "Epoch 63/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2208 - acc: 0.9131\n",
            "Epoch 64/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2203 - acc: 0.9095\n",
            "Epoch 65/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2332 - acc: 0.8997\n",
            "Epoch 66/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2152 - acc: 0.9005\n",
            "Epoch 67/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2308 - acc: 0.9042\n",
            "Epoch 68/150\n",
            "2463/2463 [==============================] - 0s 182us/step - loss: 0.2301 - acc: 0.9066\n",
            "Epoch 69/150\n",
            "2463/2463 [==============================] - 0s 188us/step - loss: 0.2250 - acc: 0.9062\n",
            "Epoch 70/150\n",
            "2463/2463 [==============================] - 0s 190us/step - loss: 0.2180 - acc: 0.9066\n",
            "Epoch 71/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2174 - acc: 0.9050\n",
            "Epoch 72/150\n",
            "2463/2463 [==============================] - 0s 192us/step - loss: 0.2193 - acc: 0.9139\n",
            "Epoch 73/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2231 - acc: 0.9046\n",
            "Epoch 74/150\n",
            "2463/2463 [==============================] - 0s 194us/step - loss: 0.2172 - acc: 0.9095\n",
            "Epoch 75/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2316 - acc: 0.9038\n",
            "Epoch 76/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2110 - acc: 0.9103\n",
            "Epoch 77/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2140 - acc: 0.9082\n",
            "Epoch 78/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2227 - acc: 0.9005\n",
            "Epoch 79/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2190 - acc: 0.9078\n",
            "Epoch 80/150\n",
            "2463/2463 [==============================] - 0s 191us/step - loss: 0.2139 - acc: 0.9082\n",
            "Epoch 81/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2239 - acc: 0.9062\n",
            "Epoch 82/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2168 - acc: 0.9135\n",
            "Epoch 83/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.2184 - acc: 0.9074\n",
            "Epoch 84/150\n",
            "2463/2463 [==============================] - 0s 191us/step - loss: 0.2278 - acc: 0.9054\n",
            "Epoch 85/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2143 - acc: 0.9139\n",
            "Epoch 86/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2282 - acc: 0.9046\n",
            "Epoch 87/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2221 - acc: 0.9038\n",
            "Epoch 88/150\n",
            "2463/2463 [==============================] - 0s 181us/step - loss: 0.2211 - acc: 0.9086\n",
            "Epoch 89/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2184 - acc: 0.9070\n",
            "Epoch 90/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2131 - acc: 0.9115\n",
            "Epoch 91/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.2158 - acc: 0.9107\n",
            "Epoch 92/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2153 - acc: 0.9095\n",
            "Epoch 93/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2107 - acc: 0.9095\n",
            "Epoch 94/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2167 - acc: 0.9070\n",
            "Epoch 95/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2071 - acc: 0.9164\n",
            "Epoch 96/150\n",
            "2463/2463 [==============================] - 0s 188us/step - loss: 0.2206 - acc: 0.9038\n",
            "Epoch 97/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2085 - acc: 0.9046\n",
            "Epoch 98/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2159 - acc: 0.9111\n",
            "Epoch 99/150\n",
            "2463/2463 [==============================] - 0s 182us/step - loss: 0.2086 - acc: 0.9103\n",
            "Epoch 100/150\n",
            "2463/2463 [==============================] - 0s 173us/step - loss: 0.2181 - acc: 0.9042\n",
            "Epoch 101/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.1971 - acc: 0.9147\n",
            "Epoch 102/150\n",
            "2463/2463 [==============================] - 0s 182us/step - loss: 0.2106 - acc: 0.9103\n",
            "Epoch 103/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2140 - acc: 0.9111\n",
            "Epoch 104/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2180 - acc: 0.9074\n",
            "Epoch 105/150\n",
            "2463/2463 [==============================] - 0s 187us/step - loss: 0.2027 - acc: 0.9160\n",
            "Epoch 106/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2044 - acc: 0.9188\n",
            "Epoch 107/150\n",
            "2463/2463 [==============================] - 0s 180us/step - loss: 0.2055 - acc: 0.9143\n",
            "Epoch 108/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.1969 - acc: 0.9188\n",
            "Epoch 109/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2125 - acc: 0.9058\n",
            "Epoch 110/150\n",
            "2463/2463 [==============================] - 0s 183us/step - loss: 0.2052 - acc: 0.9091\n",
            "Epoch 111/150\n",
            "2463/2463 [==============================] - 0s 191us/step - loss: 0.1984 - acc: 0.9164\n",
            "Epoch 112/150\n",
            "2463/2463 [==============================] - 0s 181us/step - loss: 0.2109 - acc: 0.9046\n",
            "Epoch 113/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.2176 - acc: 0.9107\n",
            "Epoch 114/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2233 - acc: 0.9115\n",
            "Epoch 115/150\n",
            "2463/2463 [==============================] - 0s 190us/step - loss: 0.1974 - acc: 0.9233\n",
            "Epoch 116/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.1987 - acc: 0.9147\n",
            "Epoch 117/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.2175 - acc: 0.9074\n",
            "Epoch 118/150\n",
            "2463/2463 [==============================] - 0s 175us/step - loss: 0.1965 - acc: 0.9160\n",
            "Epoch 119/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2129 - acc: 0.9103\n",
            "Epoch 120/150\n",
            "2463/2463 [==============================] - 0s 196us/step - loss: 0.2158 - acc: 0.9103\n",
            "Epoch 121/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.2019 - acc: 0.9180\n",
            "Epoch 122/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2173 - acc: 0.9123\n",
            "Epoch 123/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2058 - acc: 0.9164\n",
            "Epoch 124/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.2096 - acc: 0.9111\n",
            "Epoch 125/150\n",
            "2463/2463 [==============================] - 0s 191us/step - loss: 0.2102 - acc: 0.9078\n",
            "Epoch 126/150\n",
            "2463/2463 [==============================] - 0s 174us/step - loss: 0.2051 - acc: 0.9127\n",
            "Epoch 127/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2098 - acc: 0.9078\n",
            "Epoch 128/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2008 - acc: 0.9086\n",
            "Epoch 129/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.1826 - acc: 0.9151\n",
            "Epoch 130/150\n",
            "2463/2463 [==============================] - 0s 181us/step - loss: 0.1931 - acc: 0.9151\n",
            "Epoch 131/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.2104 - acc: 0.9123\n",
            "Epoch 132/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2107 - acc: 0.9151\n",
            "Epoch 133/150\n",
            "2463/2463 [==============================] - 0s 174us/step - loss: 0.1906 - acc: 0.9160\n",
            "Epoch 134/150\n",
            "2463/2463 [==============================] - 0s 185us/step - loss: 0.1979 - acc: 0.9160\n",
            "Epoch 135/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.1917 - acc: 0.9237\n",
            "Epoch 136/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.2021 - acc: 0.9180\n",
            "Epoch 137/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.1989 - acc: 0.9127\n",
            "Epoch 138/150\n",
            "2463/2463 [==============================] - 0s 178us/step - loss: 0.1968 - acc: 0.9156\n",
            "Epoch 139/150\n",
            "2463/2463 [==============================] - 0s 176us/step - loss: 0.1988 - acc: 0.9233\n",
            "Epoch 140/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2031 - acc: 0.9135\n",
            "Epoch 141/150\n",
            "2463/2463 [==============================] - 0s 181us/step - loss: 0.1851 - acc: 0.9176\n",
            "Epoch 142/150\n",
            "2463/2463 [==============================] - 0s 183us/step - loss: 0.1949 - acc: 0.9188\n",
            "Epoch 143/150\n",
            "2463/2463 [==============================] - 0s 195us/step - loss: 0.1962 - acc: 0.9184\n",
            "Epoch 144/150\n",
            "2463/2463 [==============================] - 0s 179us/step - loss: 0.1872 - acc: 0.9253\n",
            "Epoch 145/150\n",
            "2463/2463 [==============================] - 0s 184us/step - loss: 0.2017 - acc: 0.9216\n",
            "Epoch 146/150\n",
            "2463/2463 [==============================] - 0s 188us/step - loss: 0.1929 - acc: 0.9216\n",
            "Epoch 147/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.2007 - acc: 0.9192\n",
            "Epoch 148/150\n",
            "2463/2463 [==============================] - 0s 186us/step - loss: 0.1889 - acc: 0.9237\n",
            "Epoch 149/150\n",
            "2463/2463 [==============================] - 0s 172us/step - loss: 0.1991 - acc: 0.9147\n",
            "Epoch 150/150\n",
            "2463/2463 [==============================] - 0s 177us/step - loss: 0.1857 - acc: 0.9225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6eae69c978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3JbzefpjLE-",
        "colab_type": "code",
        "outputId": "c03a7eab-3eb2-48d7-dd12-397b9358d35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "_, accuracy = model.evaluate(test_scaled, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "616/616 [==============================] - 1s 861us/step\n",
            "Accuracy: 91.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksYqBXZwjO61",
        "colab_type": "code",
        "outputId": "68b00ed7-c101-428e-b340-5cffdafd680f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "_, accuracy = model.evaluate(train_scaled, y_train)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2463/2463 [==============================] - 0s 30us/step\n",
            "Accuracy: 97.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABvej76cjQQq",
        "colab_type": "code",
        "outputId": "4f77a569-f7d5-4080-ce88-ecb2d7ceacfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dropout(0.3, input_shape=(98,)))\n",
        "  model.add(Dense(98, input_dim=98, activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(250, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "keras_model = build_model()\n",
        "keras_model.fit(train_scaled, y_train, epochs=100, batch_size=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2463/2463 [==============================] - 2s 758us/step - loss: 0.3923 - acc: 0.8729\n",
            "Epoch 2/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.3478 - acc: 0.8737\n",
            "Epoch 3/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.3303 - acc: 0.8713\n",
            "Epoch 4/100\n",
            "2463/2463 [==============================] - 0s 119us/step - loss: 0.3170 - acc: 0.8745\n",
            "Epoch 5/100\n",
            "2463/2463 [==============================] - 0s 112us/step - loss: 0.3147 - acc: 0.8754\n",
            "Epoch 6/100\n",
            "2463/2463 [==============================] - 0s 115us/step - loss: 0.3004 - acc: 0.8806\n",
            "Epoch 7/100\n",
            "2463/2463 [==============================] - 0s 117us/step - loss: 0.2976 - acc: 0.8790\n",
            "Epoch 8/100\n",
            "2463/2463 [==============================] - 0s 118us/step - loss: 0.2880 - acc: 0.8843\n",
            "Epoch 9/100\n",
            "2463/2463 [==============================] - 0s 125us/step - loss: 0.2963 - acc: 0.8843\n",
            "Epoch 10/100\n",
            "2463/2463 [==============================] - 0s 117us/step - loss: 0.2914 - acc: 0.8823\n",
            "Epoch 11/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.2881 - acc: 0.8888\n",
            "Epoch 12/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.2833 - acc: 0.8814\n",
            "Epoch 13/100\n",
            "2463/2463 [==============================] - 0s 120us/step - loss: 0.2693 - acc: 0.8936\n",
            "Epoch 14/100\n",
            "2463/2463 [==============================] - 0s 115us/step - loss: 0.2739 - acc: 0.8883\n",
            "Epoch 15/100\n",
            "2463/2463 [==============================] - 0s 109us/step - loss: 0.2660 - acc: 0.8855\n",
            "Epoch 16/100\n",
            "2463/2463 [==============================] - 0s 124us/step - loss: 0.2725 - acc: 0.8904\n",
            "Epoch 17/100\n",
            "2463/2463 [==============================] - 0s 118us/step - loss: 0.2668 - acc: 0.8928\n",
            "Epoch 18/100\n",
            "2463/2463 [==============================] - 0s 111us/step - loss: 0.2692 - acc: 0.8835\n",
            "Epoch 19/100\n",
            "2463/2463 [==============================] - 0s 126us/step - loss: 0.2672 - acc: 0.8892\n",
            "Epoch 20/100\n",
            "2463/2463 [==============================] - 0s 124us/step - loss: 0.2637 - acc: 0.8839\n",
            "Epoch 21/100\n",
            "2463/2463 [==============================] - 0s 127us/step - loss: 0.2513 - acc: 0.8879\n",
            "Epoch 22/100\n",
            "2463/2463 [==============================] - 0s 129us/step - loss: 0.2547 - acc: 0.8981\n",
            "Epoch 23/100\n",
            "2463/2463 [==============================] - 0s 120us/step - loss: 0.2545 - acc: 0.8989\n",
            "Epoch 24/100\n",
            "2463/2463 [==============================] - 0s 124us/step - loss: 0.2577 - acc: 0.8859\n",
            "Epoch 25/100\n",
            "2463/2463 [==============================] - 0s 136us/step - loss: 0.2503 - acc: 0.8912\n",
            "Epoch 26/100\n",
            "2463/2463 [==============================] - 0s 125us/step - loss: 0.2473 - acc: 0.8973\n",
            "Epoch 27/100\n",
            "2463/2463 [==============================] - 0s 134us/step - loss: 0.2442 - acc: 0.8965\n",
            "Epoch 28/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.2427 - acc: 0.8920\n",
            "Epoch 29/100\n",
            "2463/2463 [==============================] - 0s 147us/step - loss: 0.2392 - acc: 0.8936\n",
            "Epoch 30/100\n",
            "2463/2463 [==============================] - 0s 133us/step - loss: 0.2414 - acc: 0.8961\n",
            "Epoch 31/100\n",
            "2463/2463 [==============================] - 0s 132us/step - loss: 0.2462 - acc: 0.8973\n",
            "Epoch 32/100\n",
            "2463/2463 [==============================] - 0s 126us/step - loss: 0.2465 - acc: 0.8924\n",
            "Epoch 33/100\n",
            "2463/2463 [==============================] - 0s 134us/step - loss: 0.2395 - acc: 0.8957\n",
            "Epoch 34/100\n",
            "2463/2463 [==============================] - 0s 128us/step - loss: 0.2416 - acc: 0.8957\n",
            "Epoch 35/100\n",
            "2463/2463 [==============================] - 0s 140us/step - loss: 0.2303 - acc: 0.9046\n",
            "Epoch 36/100\n",
            "2463/2463 [==============================] - 0s 148us/step - loss: 0.2376 - acc: 0.9001\n",
            "Epoch 37/100\n",
            "2463/2463 [==============================] - 0s 139us/step - loss: 0.2273 - acc: 0.8981\n",
            "Epoch 38/100\n",
            "2463/2463 [==============================] - 0s 133us/step - loss: 0.2167 - acc: 0.9046\n",
            "Epoch 39/100\n",
            "2463/2463 [==============================] - 0s 139us/step - loss: 0.2316 - acc: 0.9066\n",
            "Epoch 40/100\n",
            "2463/2463 [==============================] - 0s 134us/step - loss: 0.2362 - acc: 0.8952\n",
            "Epoch 41/100\n",
            "2463/2463 [==============================] - 0s 124us/step - loss: 0.2225 - acc: 0.9042\n",
            "Epoch 42/100\n",
            "2463/2463 [==============================] - 0s 127us/step - loss: 0.2411 - acc: 0.8965\n",
            "Epoch 43/100\n",
            "2463/2463 [==============================] - 0s 132us/step - loss: 0.2238 - acc: 0.9086\n",
            "Epoch 44/100\n",
            "2463/2463 [==============================] - 0s 142us/step - loss: 0.2446 - acc: 0.8924\n",
            "Epoch 45/100\n",
            "2463/2463 [==============================] - 0s 135us/step - loss: 0.2382 - acc: 0.9030\n",
            "Epoch 46/100\n",
            "2463/2463 [==============================] - 0s 134us/step - loss: 0.2285 - acc: 0.9034\n",
            "Epoch 47/100\n",
            "2463/2463 [==============================] - 0s 144us/step - loss: 0.2319 - acc: 0.8965\n",
            "Epoch 48/100\n",
            "2463/2463 [==============================] - 0s 137us/step - loss: 0.2233 - acc: 0.9082\n",
            "Epoch 49/100\n",
            "2463/2463 [==============================] - 0s 129us/step - loss: 0.2138 - acc: 0.9082\n",
            "Epoch 50/100\n",
            "2463/2463 [==============================] - 0s 125us/step - loss: 0.2319 - acc: 0.8985\n",
            "Epoch 51/100\n",
            "2463/2463 [==============================] - 0s 135us/step - loss: 0.2309 - acc: 0.9001\n",
            "Epoch 52/100\n",
            "2463/2463 [==============================] - 0s 127us/step - loss: 0.2253 - acc: 0.9042\n",
            "Epoch 53/100\n",
            "2463/2463 [==============================] - 0s 125us/step - loss: 0.2180 - acc: 0.9054\n",
            "Epoch 54/100\n",
            "2463/2463 [==============================] - 0s 123us/step - loss: 0.2242 - acc: 0.9026\n",
            "Epoch 55/100\n",
            "2463/2463 [==============================] - 0s 123us/step - loss: 0.2143 - acc: 0.9042\n",
            "Epoch 56/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.2234 - acc: 0.9034\n",
            "Epoch 57/100\n",
            "2463/2463 [==============================] - 0s 113us/step - loss: 0.2241 - acc: 0.8993\n",
            "Epoch 58/100\n",
            "2463/2463 [==============================] - 0s 119us/step - loss: 0.2104 - acc: 0.9103\n",
            "Epoch 59/100\n",
            "2463/2463 [==============================] - 0s 117us/step - loss: 0.2193 - acc: 0.9058\n",
            "Epoch 60/100\n",
            "2463/2463 [==============================] - 0s 123us/step - loss: 0.2077 - acc: 0.9123\n",
            "Epoch 61/100\n",
            "2463/2463 [==============================] - 0s 124us/step - loss: 0.2159 - acc: 0.9095\n",
            "Epoch 62/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.2045 - acc: 0.9192\n",
            "Epoch 63/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.2148 - acc: 0.9074\n",
            "Epoch 64/100\n",
            "2463/2463 [==============================] - 0s 120us/step - loss: 0.2199 - acc: 0.9050\n",
            "Epoch 65/100\n",
            "2463/2463 [==============================] - 0s 112us/step - loss: 0.2075 - acc: 0.9082\n",
            "Epoch 66/100\n",
            "2463/2463 [==============================] - 0s 134us/step - loss: 0.2152 - acc: 0.9143\n",
            "Epoch 67/100\n",
            "2463/2463 [==============================] - 0s 133us/step - loss: 0.1936 - acc: 0.9172\n",
            "Epoch 68/100\n",
            "2463/2463 [==============================] - 0s 117us/step - loss: 0.1969 - acc: 0.9184\n",
            "Epoch 69/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.2014 - acc: 0.9164\n",
            "Epoch 70/100\n",
            "2463/2463 [==============================] - 0s 121us/step - loss: 0.2168 - acc: 0.9091\n",
            "Epoch 71/100\n",
            "2463/2463 [==============================] - 0s 125us/step - loss: 0.2027 - acc: 0.9160\n",
            "Epoch 72/100\n",
            "2463/2463 [==============================] - 0s 121us/step - loss: 0.1945 - acc: 0.9192\n",
            "Epoch 73/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.1966 - acc: 0.9119\n",
            "Epoch 74/100\n",
            "2463/2463 [==============================] - 0s 118us/step - loss: 0.2221 - acc: 0.9050\n",
            "Epoch 75/100\n",
            "2463/2463 [==============================] - 0s 113us/step - loss: 0.1860 - acc: 0.9200\n",
            "Epoch 76/100\n",
            "2463/2463 [==============================] - 0s 131us/step - loss: 0.2080 - acc: 0.9099\n",
            "Epoch 77/100\n",
            "2463/2463 [==============================] - 0s 113us/step - loss: 0.2091 - acc: 0.9111\n",
            "Epoch 78/100\n",
            "2463/2463 [==============================] - 0s 121us/step - loss: 0.2036 - acc: 0.9135\n",
            "Epoch 79/100\n",
            "2463/2463 [==============================] - 0s 115us/step - loss: 0.2056 - acc: 0.9160\n",
            "Epoch 80/100\n",
            "2463/2463 [==============================] - 0s 113us/step - loss: 0.1970 - acc: 0.9180\n",
            "Epoch 81/100\n",
            "2463/2463 [==============================] - 0s 114us/step - loss: 0.2060 - acc: 0.9135\n",
            "Epoch 82/100\n",
            "2463/2463 [==============================] - 0s 121us/step - loss: 0.1914 - acc: 0.9204\n",
            "Epoch 83/100\n",
            "2463/2463 [==============================] - 0s 118us/step - loss: 0.2041 - acc: 0.9131\n",
            "Epoch 84/100\n",
            "2463/2463 [==============================] - 0s 115us/step - loss: 0.1927 - acc: 0.9204\n",
            "Epoch 85/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.1841 - acc: 0.9237\n",
            "Epoch 86/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.1988 - acc: 0.9172\n",
            "Epoch 87/100\n",
            "2463/2463 [==============================] - 0s 114us/step - loss: 0.1931 - acc: 0.9164\n",
            "Epoch 88/100\n",
            "2463/2463 [==============================] - 0s 117us/step - loss: 0.1896 - acc: 0.9192\n",
            "Epoch 89/100\n",
            "2463/2463 [==============================] - 0s 115us/step - loss: 0.1863 - acc: 0.9164\n",
            "Epoch 90/100\n",
            "2463/2463 [==============================] - 0s 122us/step - loss: 0.1949 - acc: 0.9180\n",
            "Epoch 91/100\n",
            "2463/2463 [==============================] - 0s 132us/step - loss: 0.1992 - acc: 0.9192\n",
            "Epoch 92/100\n",
            "2463/2463 [==============================] - 0s 120us/step - loss: 0.1911 - acc: 0.9172\n",
            "Epoch 93/100\n",
            "2463/2463 [==============================] - 0s 124us/step - loss: 0.1981 - acc: 0.9143\n",
            "Epoch 94/100\n",
            "2463/2463 [==============================] - 0s 119us/step - loss: 0.2003 - acc: 0.9160\n",
            "Epoch 95/100\n",
            "2463/2463 [==============================] - 0s 116us/step - loss: 0.1814 - acc: 0.9249\n",
            "Epoch 96/100\n",
            "2463/2463 [==============================] - 0s 114us/step - loss: 0.1883 - acc: 0.9168\n",
            "Epoch 97/100\n",
            "2463/2463 [==============================] - 0s 121us/step - loss: 0.1864 - acc: 0.9253\n",
            "Epoch 98/100\n",
            "2463/2463 [==============================] - 0s 114us/step - loss: 0.1861 - acc: 0.9225\n",
            "Epoch 99/100\n",
            "2463/2463 [==============================] - 0s 108us/step - loss: 0.1922 - acc: 0.9160\n",
            "Epoch 100/100\n",
            "2463/2463 [==============================] - 0s 110us/step - loss: 0.1842 - acc: 0.9220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ead82fa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gob4QiRBp1gA",
        "colab_type": "code",
        "outputId": "42d922f7-0c09-40c7-f467-c1c0d2e46a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "_, accuracy = model.evaluate(test_scaled, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "_, accuracy = model.evaluate(train_scaled, y_train)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "616/616 [==============================] - 0s 56us/step\n",
            "Accuracy: 91.23\n",
            "2463/2463 [==============================] - 0s 36us/step\n",
            "Accuracy: 97.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTaTibUGpK6n",
        "colab_type": "code",
        "outputId": "2e721e76-3323-4e43-9ada-7343cf03fdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "y_pred_keras = keras_model.predict(test_scaled).ravel()\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\n",
        "from sklearn.metrics import auc\n",
        "auc_keras = auc(fpr_keras, tpr_keras)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUuQw/YEfb2AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m6E1loapgAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}