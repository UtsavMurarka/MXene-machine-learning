{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cost_sensitive_NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuaamqHAkaO5",
        "colab_type": "code",
        "outputId": "bb5b0e7e-2d41-4914-c686-2e870eddf192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#data=pd.read_csv(\"C:\\\\Users\\\\Utsav\\\\Desktop\\\\Mxene\\\\ML_exp\", sep=',',header=0)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/UtsavMurarka/MXene-machine-learning/master/anant_data_miner/post_midsem_work/data_physical_var_new.csv\"\n",
        "\n",
        "data=pd.read_csv(url, sep=',',header=0)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>bandgap</th>\n",
              "      <th>nn1</th>\n",
              "      <th>nn2</th>\n",
              "      <th>nn3</th>\n",
              "      <th>nn4</th>\n",
              "      <th>nn5</th>\n",
              "      <th>np1</th>\n",
              "      <th>np2</th>\n",
              "      <th>np3</th>\n",
              "      <th>np4</th>\n",
              "      <th>np5</th>\n",
              "      <th>...</th>\n",
              "      <th>shell4</th>\n",
              "      <th>shell5</th>\n",
              "      <th>spheat1</th>\n",
              "      <th>spheat2</th>\n",
              "      <th>spheat3</th>\n",
              "      <th>spheat4</th>\n",
              "      <th>spheat5</th>\n",
              "      <th>mass_ms1</th>\n",
              "      <th>mass_ms2</th>\n",
              "      <th>mass_ms3</th>\n",
              "      <th>mass_ms4</th>\n",
              "      <th>mass_ms5</th>\n",
              "      <th>mol_ms1</th>\n",
              "      <th>mol_ms2</th>\n",
              "      <th>mol_ms3</th>\n",
              "      <th>mol_ms4</th>\n",
              "      <th>mol_ms5</th>\n",
              "      <th>ther_con1</th>\n",
              "      <th>ther_con2</th>\n",
              "      <th>ther_con3</th>\n",
              "      <th>ther_con4</th>\n",
              "      <th>ther_con5</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>var10</th>\n",
              "      <th>var11</th>\n",
              "      <th>var12</th>\n",
              "      <th>var13</th>\n",
              "      <th>var14</th>\n",
              "      <th>var15</th>\n",
              "      <th>var16</th>\n",
              "      <th>var17</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e-16</td>\n",
              "      <td>32.8</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.328</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0.474</td>\n",
              "      <td>44.5</td>\n",
              "      <td>44.5</td>\n",
              "      <td>-6.2</td>\n",
              "      <td>-4.9</td>\n",
              "      <td>-4.9</td>\n",
              "      <td>231.4</td>\n",
              "      <td>231.4</td>\n",
              "      <td>-7.45</td>\n",
              "      <td>-78.3</td>\n",
              "      <td>-78.300</td>\n",
              "      <td>94</td>\n",
              "      <td>94</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>258.3</td>\n",
              "      <td>140.7</td>\n",
              "      <td>140.7</td>\n",
              "      <td>0.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>0.22962</td>\n",
              "      <td>0.43402</td>\n",
              "      <td>7.145108</td>\n",
              "      <td>5.701870</td>\n",
              "      <td>2381272.203</td>\n",
              "      <td>3122884.800</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.012408</td>\n",
              "      <td>745.290</td>\n",
              "      <td>25389.72250</td>\n",
              "      <td>3930.932320</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>2.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-2.000000e-16</td>\n",
              "      <td>35.6</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.244</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.474</td>\n",
              "      <td>14.304</td>\n",
              "      <td>44.5</td>\n",
              "      <td>44.5</td>\n",
              "      <td>-6.2</td>\n",
              "      <td>-4.9</td>\n",
              "      <td>-24.8</td>\n",
              "      <td>231.4</td>\n",
              "      <td>231.4</td>\n",
              "      <td>-7.45</td>\n",
              "      <td>-78.3</td>\n",
              "      <td>-4.999</td>\n",
              "      <td>94</td>\n",
              "      <td>94</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1805</td>\n",
              "      <td>334.8</td>\n",
              "      <td>198.5</td>\n",
              "      <td>198.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>42.7</td>\n",
              "      <td>0.29255</td>\n",
              "      <td>0.32078</td>\n",
              "      <td>9.682327</td>\n",
              "      <td>9.898093</td>\n",
              "      <td>2580607.337</td>\n",
              "      <td>3428851.696</td>\n",
              "      <td>34.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>38.010333</td>\n",
              "      <td>1018.517</td>\n",
              "      <td>21404.72830</td>\n",
              "      <td>3928.950830</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>2.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000e-16</td>\n",
              "      <td>-2.000000e-16</td>\n",
              "      <td>33.1</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.330</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>45</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.474</td>\n",
              "      <td>44.5</td>\n",
              "      <td>44.5</td>\n",
              "      <td>-6.2</td>\n",
              "      <td>-7.2</td>\n",
              "      <td>-4.9</td>\n",
              "      <td>231.4</td>\n",
              "      <td>231.4</td>\n",
              "      <td>-7.45</td>\n",
              "      <td>-51.5</td>\n",
              "      <td>-78.300</td>\n",
              "      <td>94</td>\n",
              "      <td>94</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>207.0</td>\n",
              "      <td>113.7</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>0.25133</td>\n",
              "      <td>0.50222</td>\n",
              "      <td>8.639936</td>\n",
              "      <td>9.891952</td>\n",
              "      <td>2452493.793</td>\n",
              "      <td>3209994.998</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.012320</td>\n",
              "      <td>768.773</td>\n",
              "      <td>23683.50050</td>\n",
              "      <td>3934.574869</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-1.000000e-16</td>\n",
              "      <td>34.6</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.328</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.479</td>\n",
              "      <td>44.5</td>\n",
              "      <td>44.5</td>\n",
              "      <td>-6.2</td>\n",
              "      <td>-7.2</td>\n",
              "      <td>-7.2</td>\n",
              "      <td>231.4</td>\n",
              "      <td>231.4</td>\n",
              "      <td>-7.45</td>\n",
              "      <td>-51.5</td>\n",
              "      <td>-51.500</td>\n",
              "      <td>94</td>\n",
              "      <td>94</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>82.8</td>\n",
              "      <td>54.3</td>\n",
              "      <td>54.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>31.5</td>\n",
              "      <td>0.27135</td>\n",
              "      <td>0.56642</td>\n",
              "      <td>10.001639</td>\n",
              "      <td>13.110595</td>\n",
              "      <td>2522836.664</td>\n",
              "      <td>3296242.342</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.012230</td>\n",
              "      <td>791.727</td>\n",
              "      <td>21905.45450</td>\n",
              "      <td>3938.216184</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000e-16</td>\n",
              "      <td>36.4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.369</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.245</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.479</td>\n",
              "      <td>14.304</td>\n",
              "      <td>44.5</td>\n",
              "      <td>44.5</td>\n",
              "      <td>-6.2</td>\n",
              "      <td>-7.2</td>\n",
              "      <td>-24.8</td>\n",
              "      <td>231.4</td>\n",
              "      <td>231.4</td>\n",
              "      <td>-7.45</td>\n",
              "      <td>-51.5</td>\n",
              "      <td>-4.999</td>\n",
              "      <td>94</td>\n",
              "      <td>94</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.1805</td>\n",
              "      <td>162.0</td>\n",
              "      <td>110.3</td>\n",
              "      <td>110.3</td>\n",
              "      <td>1.7</td>\n",
              "      <td>42.7</td>\n",
              "      <td>0.31023</td>\n",
              "      <td>0.40418</td>\n",
              "      <td>10.971248</td>\n",
              "      <td>13.115764</td>\n",
              "      <td>2649467.851</td>\n",
              "      <td>3513066.327</td>\n",
              "      <td>9.2</td>\n",
              "      <td>1.7</td>\n",
              "      <td>38.003330</td>\n",
              "      <td>1037.423</td>\n",
              "      <td>19502.05962</td>\n",
              "      <td>3932.594051</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 133 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     f1   f2   f3    f4  ...     var15        var16        var17  label\n",
              "0  3.26  0.0  0.0 -1.63  ...   745.290  25389.72250  3930.932320      0\n",
              "1  3.13  0.0  0.0 -1.56  ...  1018.517  21404.72830  3928.950830      0\n",
              "2  3.24  0.0  0.0 -1.62  ...   768.773  23683.50050  3934.574869      0\n",
              "3  3.18  0.0  0.0 -1.59  ...   791.727  21905.45450  3938.216184      0\n",
              "4  3.10  0.0  0.0 -1.55  ...  1037.423  19502.05962  3932.594051      0\n",
              "\n",
              "[5 rows x 133 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEjSKN64o-Rc",
        "colab_type": "code",
        "outputId": "626422ac-c73d-4d76-9746-f2795836a087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#drop band gap column\n",
        "data = data.drop('bandgap', axis=1)\n",
        "print(np.shape(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3079, 132)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z7Xlf-opZvm",
        "colab_type": "code",
        "outputId": "db9ba14f-4d8e-44a3-f046-2608dd6696f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "mxene=data.values\n",
        "mxene=np.array(mxene)\n",
        "#split the data\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "train=np.array(train)\n",
        "test=np.array(test)\n",
        "NegativeCount=0\n",
        "PositiveCount=0\n",
        "for i in range(len(train)):\n",
        "    if train[i][131]==0 :\n",
        "        NegativeCount=NegativeCount+1\n",
        "    if train[i][131]==1:\n",
        "        PositiveCount=PositiveCount+1\n",
        "print(NegativeCount)\n",
        "print(PositiveCount)\n",
        "print(PositiveCount+NegativeCount)\n",
        "positives=np.zeros((PositiveCount,132))\n",
        "negatives=np.zeros((NegativeCount,132))\n",
        "    \n",
        "j=0\n",
        "k=0\n",
        "    \n",
        "for i in range(len(train)):\n",
        "\n",
        "    if(train[i,131] == 1):\n",
        "        positives[j,:] = train[i,:]\n",
        "        j=j+1\n",
        "    if(train[i,131] == 0):\n",
        "        negatives[k,:] = train[i,:]\n",
        "        k=k+1\n",
        "\n",
        "print(np.shape(train))\n",
        "print(np.shape(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2159\n",
            "304\n",
            "2463\n",
            "(2463, 132)\n",
            "(616, 132)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDyJwzy4pbTE",
        "colab_type": "code",
        "outputId": "9cf44001-dee3-4e26-d2d8-c795304e5374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_train=np.zeros(len(train))\n",
        "y_test=np.zeros(len(test))\n",
        "\n",
        "for i in range(len(test)):\n",
        "    y_test[i]=test[i][131]\n",
        "    test[i][131]=1\n",
        "\n",
        "for i in range(len(train)):\n",
        "    y_train[i]=train[i][131]\n",
        "    train[i][131]=1\n",
        "\n",
        "train = np.delete(train, 131, 1)\n",
        "test = np.delete(test, 131, 1)\n",
        "\n",
        "train = np.delete(train, 130, 1)\n",
        "test = np.delete(test, 130, 1)\n",
        "\n",
        "print(np.shape(train))\n",
        "print(np.shape(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2463, 130)\n",
            "(616, 130)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zIBXfBspdSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train = scaler.fit_transform(train)\n",
        "test = scaler.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDMDy0DBpe34",
        "colab_type": "code",
        "outputId": "ff2ae477-c5fc-40bd-e606-6f444809a4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dropout(0.1, input_shape=(130,)))\n",
        "  model.add(Dense(130, input_dim=130, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(250, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "weights = {0:2159 ,1: 304}\n",
        "keras_model = build_model()\n",
        "keras_model.fit(train, y_train, epochs=100, batch_size=20, verbose=1, class_weight = weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2463/2463 [==============================] - 0s 150us/step - loss: 208.5045 - accuracy: 0.8762\n",
            "Epoch 2/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 174.3323 - accuracy: 0.8766\n",
            "Epoch 3/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 161.9504 - accuracy: 0.8766\n",
            "Epoch 4/100\n",
            "2463/2463 [==============================] - 0s 99us/step - loss: 162.6739 - accuracy: 0.8766\n",
            "Epoch 5/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 156.2256 - accuracy: 0.8758\n",
            "Epoch 6/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 151.3662 - accuracy: 0.8766\n",
            "Epoch 7/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 143.4219 - accuracy: 0.8770\n",
            "Epoch 8/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 148.1467 - accuracy: 0.8762\n",
            "Epoch 9/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 140.7558 - accuracy: 0.8770\n",
            "Epoch 10/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 139.1948 - accuracy: 0.8774\n",
            "Epoch 11/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 137.7367 - accuracy: 0.8766\n",
            "Epoch 12/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 128.1756 - accuracy: 0.8774\n",
            "Epoch 13/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 129.9342 - accuracy: 0.8778\n",
            "Epoch 14/100\n",
            "2463/2463 [==============================] - 0s 93us/step - loss: 133.6732 - accuracy: 0.8778\n",
            "Epoch 15/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 127.1177 - accuracy: 0.8786\n",
            "Epoch 16/100\n",
            "2463/2463 [==============================] - 0s 93us/step - loss: 123.9265 - accuracy: 0.8794\n",
            "Epoch 17/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 123.2593 - accuracy: 0.8786\n",
            "Epoch 18/100\n",
            "2463/2463 [==============================] - 0s 100us/step - loss: 116.8818 - accuracy: 0.8802\n",
            "Epoch 19/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 116.2650 - accuracy: 0.8802\n",
            "Epoch 20/100\n",
            "2463/2463 [==============================] - 0s 100us/step - loss: 116.4483 - accuracy: 0.8802\n",
            "Epoch 21/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 117.3996 - accuracy: 0.8810\n",
            "Epoch 22/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 114.2505 - accuracy: 0.8859\n",
            "Epoch 23/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 114.0243 - accuracy: 0.8916\n",
            "Epoch 24/100\n",
            "2463/2463 [==============================] - 0s 94us/step - loss: 112.7600 - accuracy: 0.8928\n",
            "Epoch 25/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 107.2719 - accuracy: 0.8879\n",
            "Epoch 26/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 108.9967 - accuracy: 0.8932\n",
            "Epoch 27/100\n",
            "2463/2463 [==============================] - 0s 93us/step - loss: 106.1853 - accuracy: 0.8997\n",
            "Epoch 28/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 103.9277 - accuracy: 0.8965\n",
            "Epoch 29/100\n",
            "2463/2463 [==============================] - 0s 105us/step - loss: 109.7262 - accuracy: 0.8957\n",
            "Epoch 30/100\n",
            "2463/2463 [==============================] - 0s 102us/step - loss: 105.3402 - accuracy: 0.8993\n",
            "Epoch 31/100\n",
            "2463/2463 [==============================] - 0s 99us/step - loss: 105.7351 - accuracy: 0.8969\n",
            "Epoch 32/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 101.4833 - accuracy: 0.9013\n",
            "Epoch 33/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 106.0044 - accuracy: 0.8989\n",
            "Epoch 34/100\n",
            "2463/2463 [==============================] - 0s 100us/step - loss: 102.1102 - accuracy: 0.9017\n",
            "Epoch 35/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 96.1119 - accuracy: 0.9050\n",
            "Epoch 36/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 100.0411 - accuracy: 0.9123\n",
            "Epoch 37/100\n",
            "2463/2463 [==============================] - 0s 103us/step - loss: 97.1238 - accuracy: 0.9123\n",
            "Epoch 38/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 104.3742 - accuracy: 0.9054\n",
            "Epoch 39/100\n",
            "2463/2463 [==============================] - 0s 99us/step - loss: 99.9774 - accuracy: 0.8997\n",
            "Epoch 40/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 89.1194 - accuracy: 0.9127\n",
            "Epoch 41/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 90.4630 - accuracy: 0.9160\n",
            "Epoch 42/100\n",
            "2463/2463 [==============================] - 0s 94us/step - loss: 97.0029 - accuracy: 0.9078\n",
            "Epoch 43/100\n",
            "2463/2463 [==============================] - 0s 102us/step - loss: 93.6948 - accuracy: 0.9111\n",
            "Epoch 44/100\n",
            "2463/2463 [==============================] - 0s 94us/step - loss: 92.0212 - accuracy: 0.9115\n",
            "Epoch 45/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 91.0626 - accuracy: 0.9237\n",
            "Epoch 46/100\n",
            "2463/2463 [==============================] - 0s 102us/step - loss: 94.3824 - accuracy: 0.9168\n",
            "Epoch 47/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 91.9094 - accuracy: 0.9172\n",
            "Epoch 48/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 92.6941 - accuracy: 0.9200\n",
            "Epoch 49/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 94.2470 - accuracy: 0.9164\n",
            "Epoch 50/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 92.6605 - accuracy: 0.9192\n",
            "Epoch 51/100\n",
            "2463/2463 [==============================] - 0s 102us/step - loss: 85.6611 - accuracy: 0.9200\n",
            "Epoch 52/100\n",
            "2463/2463 [==============================] - 0s 110us/step - loss: 89.4926 - accuracy: 0.9119\n",
            "Epoch 53/100\n",
            "2463/2463 [==============================] - 0s 108us/step - loss: 85.5919 - accuracy: 0.9233\n",
            "Epoch 54/100\n",
            "2463/2463 [==============================] - 0s 104us/step - loss: 83.9092 - accuracy: 0.9253\n",
            "Epoch 55/100\n",
            "2463/2463 [==============================] - 0s 111us/step - loss: 94.6331 - accuracy: 0.9192\n",
            "Epoch 56/100\n",
            "2463/2463 [==============================] - 0s 109us/step - loss: 85.4440 - accuracy: 0.9249\n",
            "Epoch 57/100\n",
            "2463/2463 [==============================] - 0s 105us/step - loss: 83.4492 - accuracy: 0.9229\n",
            "Epoch 58/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 79.4327 - accuracy: 0.9289\n",
            "Epoch 59/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 77.6235 - accuracy: 0.9298\n",
            "Epoch 60/100\n",
            "2463/2463 [==============================] - 0s 101us/step - loss: 79.1076 - accuracy: 0.9289\n",
            "Epoch 61/100\n",
            "2463/2463 [==============================] - 0s 112us/step - loss: 84.2823 - accuracy: 0.9269\n",
            "Epoch 62/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 83.2186 - accuracy: 0.9233\n",
            "Epoch 63/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 78.7259 - accuracy: 0.9261\n",
            "Epoch 64/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 72.5489 - accuracy: 0.9371\n",
            "Epoch 65/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 80.9716 - accuracy: 0.9334\n",
            "Epoch 66/100\n",
            "2463/2463 [==============================] - 0s 100us/step - loss: 79.5957 - accuracy: 0.9314\n",
            "Epoch 67/100\n",
            "2463/2463 [==============================] - 0s 105us/step - loss: 82.6230 - accuracy: 0.9310\n",
            "Epoch 68/100\n",
            "2463/2463 [==============================] - 0s 101us/step - loss: 79.7255 - accuracy: 0.9281\n",
            "Epoch 69/100\n",
            "2463/2463 [==============================] - 0s 100us/step - loss: 82.7161 - accuracy: 0.9289\n",
            "Epoch 70/100\n",
            "2463/2463 [==============================] - 0s 99us/step - loss: 74.4730 - accuracy: 0.9294\n",
            "Epoch 71/100\n",
            "2463/2463 [==============================] - 0s 101us/step - loss: 75.9189 - accuracy: 0.9318\n",
            "Epoch 72/100\n",
            "2463/2463 [==============================] - 0s 105us/step - loss: 78.6913 - accuracy: 0.9273\n",
            "Epoch 73/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 81.6755 - accuracy: 0.9330\n",
            "Epoch 74/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 78.3191 - accuracy: 0.9314\n",
            "Epoch 75/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 72.5432 - accuracy: 0.9326\n",
            "Epoch 76/100\n",
            "2463/2463 [==============================] - 0s 93us/step - loss: 74.1961 - accuracy: 0.9383\n",
            "Epoch 77/100\n",
            "2463/2463 [==============================] - 0s 99us/step - loss: 73.8620 - accuracy: 0.9318\n",
            "Epoch 78/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 73.1115 - accuracy: 0.9354\n",
            "Epoch 79/100\n",
            "2463/2463 [==============================] - 0s 93us/step - loss: 70.2341 - accuracy: 0.9387\n",
            "Epoch 80/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 79.5810 - accuracy: 0.9359\n",
            "Epoch 81/100\n",
            "2463/2463 [==============================] - 0s 94us/step - loss: 74.0115 - accuracy: 0.9346\n",
            "Epoch 82/100\n",
            "2463/2463 [==============================] - 0s 96us/step - loss: 71.4960 - accuracy: 0.9346\n",
            "Epoch 83/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 69.8737 - accuracy: 0.9310\n",
            "Epoch 84/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 70.7315 - accuracy: 0.9395\n",
            "Epoch 85/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 83.2649 - accuracy: 0.9298\n",
            "Epoch 86/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 67.3176 - accuracy: 0.9367\n",
            "Epoch 87/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 70.4654 - accuracy: 0.9411\n",
            "Epoch 88/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 68.1674 - accuracy: 0.9371\n",
            "Epoch 89/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 74.2723 - accuracy: 0.9375\n",
            "Epoch 90/100\n",
            "2463/2463 [==============================] - 0s 102us/step - loss: 69.1358 - accuracy: 0.9346\n",
            "Epoch 91/100\n",
            "2463/2463 [==============================] - 0s 105us/step - loss: 70.3106 - accuracy: 0.9395\n",
            "Epoch 92/100\n",
            "2463/2463 [==============================] - 0s 102us/step - loss: 69.7560 - accuracy: 0.9395\n",
            "Epoch 93/100\n",
            "2463/2463 [==============================] - 0s 109us/step - loss: 70.1010 - accuracy: 0.9432\n",
            "Epoch 94/100\n",
            "2463/2463 [==============================] - 0s 100us/step - loss: 60.2027 - accuracy: 0.9448\n",
            "Epoch 95/100\n",
            "2463/2463 [==============================] - 0s 94us/step - loss: 73.8902 - accuracy: 0.9363\n",
            "Epoch 96/100\n",
            "2463/2463 [==============================] - 0s 97us/step - loss: 65.6863 - accuracy: 0.9497\n",
            "Epoch 97/100\n",
            "2463/2463 [==============================] - 0s 94us/step - loss: 76.8829 - accuracy: 0.9346\n",
            "Epoch 98/100\n",
            "2463/2463 [==============================] - 0s 98us/step - loss: 73.3533 - accuracy: 0.9330\n",
            "Epoch 99/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 67.8385 - accuracy: 0.9428\n",
            "Epoch 100/100\n",
            "2463/2463 [==============================] - 0s 95us/step - loss: 64.8892 - accuracy: 0.9399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f32f3fd27b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNKWQb0tv79p",
        "colab_type": "code",
        "outputId": "2321f346-c6b8-40be-d6e8-e27368b7f225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "_, accuracy = keras_model.evaluate(test, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "_, accuracy = keras_model.evaluate(train, y_train)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "616/616 [==============================] - 0s 77us/step\n",
            "Accuracy: 93.67\n",
            "2463/2463 [==============================] - 0s 28us/step\n",
            "Accuracy: 98.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz7Cp2AAwSag",
        "colab_type": "code",
        "outputId": "ce3772b7-b4bc-4bef-9872-fdd4be6b23db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "y_pred_keras = keras_model.predict(test).ravel()\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\n",
        "from sklearn.metrics import auc\n",
        "auc_keras = auc(fpr_keras, tpr_keras)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2jtl_tywn9l",
        "colab_type": "code",
        "outputId": "ee3c25e6-327b-401f-dcd1-39dc58e23572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "keras_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_24 (Dropout)         (None, 130)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 130)               17030     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 130)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 250)               32750     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 50,031\n",
            "Trainable params: 50,031\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIk4EfsI67RI",
        "colab_type": "code",
        "outputId": "c172b31c-5a8a-415e-a2b6-f3f2cbe38659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_pred = keras_model.predict(test)\n",
        "for i in range(len(y_pred)):\n",
        "  if(y_pred[i]>0.5):\n",
        "    y_pred[i]=1\n",
        "  else:\n",
        "    y_pred[i]=0\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[526   1]\n",
            " [ 38  51]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2FQWXi27xp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}